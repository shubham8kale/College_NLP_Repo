{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "topic modelling.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laa6_-faH2WO",
        "outputId": "9816af6d-10c3-4e78-ea44-a1be24bf4cc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> all\n",
            "    Downloading collection 'all'\n",
            "       | \n",
            "       | Downloading package abc to /root/nltk_data...\n",
            "       |   Unzipping corpora/abc.zip.\n",
            "       | Downloading package alpino to /root/nltk_data...\n",
            "       |   Unzipping corpora/alpino.zip.\n",
            "       | Downloading package averaged_perceptron_tagger to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "       | Downloading package averaged_perceptron_tagger_ru to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger_ru.zip.\n",
            "       | Downloading package basque_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/basque_grammars.zip.\n",
            "       | Downloading package biocreative_ppi to /root/nltk_data...\n",
            "       |   Unzipping corpora/biocreative_ppi.zip.\n",
            "       | Downloading package bllip_wsj_no_aux to /root/nltk_data...\n",
            "       |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "       | Downloading package book_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/book_grammars.zip.\n",
            "       | Downloading package brown to /root/nltk_data...\n",
            "       |   Unzipping corpora/brown.zip.\n",
            "       | Downloading package brown_tei to /root/nltk_data...\n",
            "       |   Unzipping corpora/brown_tei.zip.\n",
            "       | Downloading package cess_cat to /root/nltk_data...\n",
            "       |   Unzipping corpora/cess_cat.zip.\n",
            "       | Downloading package cess_esp to /root/nltk_data...\n",
            "       |   Unzipping corpora/cess_esp.zip.\n",
            "       | Downloading package chat80 to /root/nltk_data...\n",
            "       |   Unzipping corpora/chat80.zip.\n",
            "       | Downloading package city_database to /root/nltk_data...\n",
            "       |   Unzipping corpora/city_database.zip.\n",
            "       | Downloading package cmudict to /root/nltk_data...\n",
            "       |   Unzipping corpora/cmudict.zip.\n",
            "       | Downloading package comparative_sentences to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping corpora/comparative_sentences.zip.\n",
            "       | Downloading package comtrans to /root/nltk_data...\n",
            "       | Downloading package conll2000 to /root/nltk_data...\n",
            "       |   Unzipping corpora/conll2000.zip.\n",
            "       | Downloading package conll2002 to /root/nltk_data...\n",
            "       |   Unzipping corpora/conll2002.zip.\n",
            "       | Downloading package conll2007 to /root/nltk_data...\n",
            "       | Downloading package crubadan to /root/nltk_data...\n",
            "       |   Unzipping corpora/crubadan.zip.\n",
            "       | Downloading package dependency_treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/dependency_treebank.zip.\n",
            "       | Downloading package dolch to /root/nltk_data...\n",
            "       |   Unzipping corpora/dolch.zip.\n",
            "       | Downloading package europarl_raw to /root/nltk_data...\n",
            "       |   Unzipping corpora/europarl_raw.zip.\n",
            "       | Downloading package extended_omw to /root/nltk_data...\n",
            "       |   Unzipping corpora/extended_omw.zip.\n",
            "       | Downloading package floresta to /root/nltk_data...\n",
            "       |   Unzipping corpora/floresta.zip.\n",
            "       | Downloading package framenet_v15 to /root/nltk_data...\n",
            "       |   Unzipping corpora/framenet_v15.zip.\n",
            "       | Downloading package framenet_v17 to /root/nltk_data...\n",
            "       |   Unzipping corpora/framenet_v17.zip.\n",
            "       | Downloading package gazetteers to /root/nltk_data...\n",
            "       |   Unzipping corpora/gazetteers.zip.\n",
            "       | Downloading package genesis to /root/nltk_data...\n",
            "       |   Unzipping corpora/genesis.zip.\n",
            "       | Downloading package gutenberg to /root/nltk_data...\n",
            "       |   Unzipping corpora/gutenberg.zip.\n",
            "       | Downloading package ieer to /root/nltk_data...\n",
            "       |   Unzipping corpora/ieer.zip.\n",
            "       | Downloading package inaugural to /root/nltk_data...\n",
            "       |   Unzipping corpora/inaugural.zip.\n",
            "       | Downloading package indian to /root/nltk_data...\n",
            "       |   Unzipping corpora/indian.zip.\n",
            "       | Downloading package jeita to /root/nltk_data...\n",
            "       | Downloading package kimmo to /root/nltk_data...\n",
            "       |   Unzipping corpora/kimmo.zip.\n",
            "       | Downloading package knbc to /root/nltk_data...\n",
            "       | Downloading package large_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/large_grammars.zip.\n",
            "       | Downloading package lin_thesaurus to /root/nltk_data...\n",
            "       |   Unzipping corpora/lin_thesaurus.zip.\n",
            "       | Downloading package mac_morpho to /root/nltk_data...\n",
            "       |   Unzipping corpora/mac_morpho.zip.\n",
            "       | Downloading package machado to /root/nltk_data...\n",
            "       | Downloading package masc_tagged to /root/nltk_data...\n",
            "       | Downloading package maxent_ne_chunker to /root/nltk_data...\n",
            "       |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "       | Downloading package maxent_treebank_pos_tagger to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "       | Downloading package moses_sample to /root/nltk_data...\n",
            "       |   Unzipping models/moses_sample.zip.\n",
            "       | Downloading package movie_reviews to /root/nltk_data...\n",
            "       |   Unzipping corpora/movie_reviews.zip.\n",
            "       | Downloading package mte_teip5 to /root/nltk_data...\n",
            "       |   Unzipping corpora/mte_teip5.zip.\n",
            "       | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "       |   Unzipping misc/mwa_ppdb.zip.\n",
            "       | Downloading package names to /root/nltk_data...\n",
            "       |   Unzipping corpora/names.zip.\n",
            "       | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "       | Downloading package nonbreaking_prefixes to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "       | Downloading package nps_chat to /root/nltk_data...\n",
            "       |   Unzipping corpora/nps_chat.zip.\n",
            "       | Downloading package omw to /root/nltk_data...\n",
            "       |   Unzipping corpora/omw.zip.\n",
            "       | Downloading package omw-1.4 to /root/nltk_data...\n",
            "       |   Unzipping corpora/omw-1.4.zip.\n",
            "       | Downloading package opinion_lexicon to /root/nltk_data...\n",
            "       |   Unzipping corpora/opinion_lexicon.zip.\n",
            "       | Downloading package panlex_swadesh to /root/nltk_data...\n",
            "       | Downloading package paradigms to /root/nltk_data...\n",
            "       |   Unzipping corpora/paradigms.zip.\n",
            "       | Downloading package pe08 to /root/nltk_data...\n",
            "       |   Unzipping corpora/pe08.zip.\n",
            "       | Downloading package perluniprops to /root/nltk_data...\n",
            "       |   Unzipping misc/perluniprops.zip.\n",
            "       | Downloading package pil to /root/nltk_data...\n",
            "       |   Unzipping corpora/pil.zip.\n",
            "       | Downloading package pl196x to /root/nltk_data...\n",
            "       |   Unzipping corpora/pl196x.zip.\n",
            "       | Downloading package porter_test to /root/nltk_data...\n",
            "       |   Unzipping stemmers/porter_test.zip.\n",
            "       | Downloading package ppattach to /root/nltk_data...\n",
            "       |   Unzipping corpora/ppattach.zip.\n",
            "       | Downloading package problem_reports to /root/nltk_data...\n",
            "       |   Unzipping corpora/problem_reports.zip.\n",
            "       | Downloading package product_reviews_1 to /root/nltk_data...\n",
            "       |   Unzipping corpora/product_reviews_1.zip.\n",
            "       | Downloading package product_reviews_2 to /root/nltk_data...\n",
            "       |   Unzipping corpora/product_reviews_2.zip.\n",
            "       | Downloading package propbank to /root/nltk_data...\n",
            "       | Downloading package pros_cons to /root/nltk_data...\n",
            "       |   Unzipping corpora/pros_cons.zip.\n",
            "       | Downloading package ptb to /root/nltk_data...\n",
            "       |   Unzipping corpora/ptb.zip.\n",
            "       | Downloading package punkt to /root/nltk_data...\n",
            "       |   Unzipping tokenizers/punkt.zip.\n",
            "       | Downloading package qc to /root/nltk_data...\n",
            "       |   Unzipping corpora/qc.zip.\n",
            "       | Downloading package reuters to /root/nltk_data...\n",
            "       | Downloading package rslp to /root/nltk_data...\n",
            "       |   Unzipping stemmers/rslp.zip.\n",
            "       | Downloading package rte to /root/nltk_data...\n",
            "       |   Unzipping corpora/rte.zip.\n",
            "       | Downloading package sample_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/sample_grammars.zip.\n",
            "       | Downloading package semcor to /root/nltk_data...\n",
            "       | Downloading package senseval to /root/nltk_data...\n",
            "       |   Unzipping corpora/senseval.zip.\n",
            "       | Downloading package sentence_polarity to /root/nltk_data...\n",
            "       |   Unzipping corpora/sentence_polarity.zip.\n",
            "       | Downloading package sentiwordnet to /root/nltk_data...\n",
            "       |   Unzipping corpora/sentiwordnet.zip.\n",
            "       | Downloading package shakespeare to /root/nltk_data...\n",
            "       |   Unzipping corpora/shakespeare.zip.\n",
            "       | Downloading package sinica_treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/sinica_treebank.zip.\n",
            "       | Downloading package smultron to /root/nltk_data...\n",
            "       |   Unzipping corpora/smultron.zip.\n",
            "       | Downloading package snowball_data to /root/nltk_data...\n",
            "       | Downloading package spanish_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/spanish_grammars.zip.\n",
            "       | Downloading package state_union to /root/nltk_data...\n",
            "       |   Unzipping corpora/state_union.zip.\n",
            "       | Downloading package stopwords to /root/nltk_data...\n",
            "       |   Unzipping corpora/stopwords.zip.\n",
            "       | Downloading package subjectivity to /root/nltk_data...\n",
            "       |   Unzipping corpora/subjectivity.zip.\n",
            "       | Downloading package swadesh to /root/nltk_data...\n",
            "       |   Unzipping corpora/swadesh.zip.\n",
            "       | Downloading package switchboard to /root/nltk_data...\n",
            "       |   Unzipping corpora/switchboard.zip.\n",
            "       | Downloading package tagsets to /root/nltk_data...\n",
            "       |   Unzipping help/tagsets.zip.\n",
            "       | Downloading package timit to /root/nltk_data...\n",
            "       |   Unzipping corpora/timit.zip.\n",
            "       | Downloading package toolbox to /root/nltk_data...\n",
            "       |   Unzipping corpora/toolbox.zip.\n",
            "       | Downloading package treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/treebank.zip.\n",
            "       | Downloading package twitter_samples to /root/nltk_data...\n",
            "       |   Unzipping corpora/twitter_samples.zip.\n",
            "       | Downloading package udhr to /root/nltk_data...\n",
            "       |   Unzipping corpora/udhr.zip.\n",
            "       | Downloading package udhr2 to /root/nltk_data...\n",
            "       |   Unzipping corpora/udhr2.zip.\n",
            "       | Downloading package unicode_samples to /root/nltk_data...\n",
            "       |   Unzipping corpora/unicode_samples.zip.\n",
            "       | Downloading package universal_tagset to /root/nltk_data...\n",
            "       |   Unzipping taggers/universal_tagset.zip.\n",
            "       | Downloading package universal_treebanks_v20 to\n",
            "       |     /root/nltk_data...\n",
            "       | Downloading package vader_lexicon to /root/nltk_data...\n",
            "       | Downloading package verbnet to /root/nltk_data...\n",
            "       |   Unzipping corpora/verbnet.zip.\n",
            "       | Downloading package verbnet3 to /root/nltk_data...\n",
            "       |   Unzipping corpora/verbnet3.zip.\n",
            "       | Downloading package webtext to /root/nltk_data...\n",
            "       |   Unzipping corpora/webtext.zip.\n",
            "       | Downloading package wmt15_eval to /root/nltk_data...\n",
            "       |   Unzipping models/wmt15_eval.zip.\n",
            "       | Downloading package word2vec_sample to /root/nltk_data...\n",
            "       |   Unzipping models/word2vec_sample.zip.\n",
            "       | Downloading package wordnet to /root/nltk_data...\n",
            "       |   Unzipping corpora/wordnet.zip.\n",
            "       | Downloading package wordnet2021 to /root/nltk_data...\n",
            "       |   Unzipping corpora/wordnet2021.zip.\n",
            "       | Downloading package wordnet31 to /root/nltk_data...\n",
            "       |   Unzipping corpora/wordnet31.zip.\n",
            "       | Downloading package wordnet_ic to /root/nltk_data...\n",
            "       |   Unzipping corpora/wordnet_ic.zip.\n",
            "       | Downloading package words to /root/nltk_data...\n",
            "       |   Unzipping corpora/words.zip.\n",
            "       | Downloading package ycoe to /root/nltk_data...\n",
            "       |   Unzipping corpora/ycoe.zip.\n",
            "       | \n",
            "     Done downloading collection all\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download()\n",
        "from nltk.corpus import stopwords  #stopwords\n",
        "from nltk.stem import WordNetLemmatizer  \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "stop_words=set(nltk.corpus.stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/MyDrive/papers.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "bexqxBETH8xG",
        "outputId": "58a308b9-30a4-482f-c396-37f52be10cc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-955cff1d-6054-4144-b8fd-c14b3181bb11\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>year</th>\n",
              "      <th>title</th>\n",
              "      <th>event_type</th>\n",
              "      <th>pdf_name</th>\n",
              "      <th>abstract</th>\n",
              "      <th>paper_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1987</td>\n",
              "      <td>Self-Organization of Associative Database and ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1-self-organization-of-associative-database-an...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10</td>\n",
              "      <td>1987</td>\n",
              "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100</td>\n",
              "      <td>1988</td>\n",
              "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100-storing-covariance-by-the-associative-long...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000</td>\n",
              "      <td>1994</td>\n",
              "      <td>Bayesian Query Construction for Neural Network...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1000-bayesian-query-construction-for-neural-ne...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1001</td>\n",
              "      <td>1994</td>\n",
              "      <td>Neural Network Ensembles, Cross Validation, an...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1001-neural-network-ensembles-cross-validation...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-955cff1d-6054-4144-b8fd-c14b3181bb11')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-955cff1d-6054-4144-b8fd-c14b3181bb11 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-955cff1d-6054-4144-b8fd-c14b3181bb11');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     id  year                                              title event_type  \\\n",
              "0     1  1987  Self-Organization of Associative Database and ...        NaN   \n",
              "1    10  1987  A Mean Field Theory of Layer IV of Visual Cort...        NaN   \n",
              "2   100  1988  Storing Covariance by the Associative Long-Ter...        NaN   \n",
              "3  1000  1994  Bayesian Query Construction for Neural Network...        NaN   \n",
              "4  1001  1994  Neural Network Ensembles, Cross Validation, an...        NaN   \n",
              "\n",
              "                                            pdf_name          abstract  \\\n",
              "0  1-self-organization-of-associative-database-an...  Abstract Missing   \n",
              "1  10-a-mean-field-theory-of-layer-iv-of-visual-c...  Abstract Missing   \n",
              "2  100-storing-covariance-by-the-associative-long...  Abstract Missing   \n",
              "3  1000-bayesian-query-construction-for-neural-ne...  Abstract Missing   \n",
              "4  1001-neural-network-ensembles-cross-validation...  Abstract Missing   \n",
              "\n",
              "                                          paper_text  \n",
              "0  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n",
              "1  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n",
              "2  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...  \n",
              "3  Bayesian Query Construction for Neural\\nNetwor...  \n",
              "4  Neural Network Ensembles, Cross\\nValidation, a...  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#df = df['paper_text']"
      ],
      "metadata": {
        "id": "iDJ_1Zy3JE92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "sSfZl4Z5JRb3",
        "outputId": "5a5eeeb6-bb50-4b60-ec92-be0818c39d55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-3f51c9e2-c12c-4ed5-9e66-1f1d2f989b3d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>year</th>\n",
              "      <th>title</th>\n",
              "      <th>event_type</th>\n",
              "      <th>pdf_name</th>\n",
              "      <th>abstract</th>\n",
              "      <th>paper_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1987</td>\n",
              "      <td>Self-Organization of Associative Database and ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1-self-organization-of-associative-database-an...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10</td>\n",
              "      <td>1987</td>\n",
              "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100</td>\n",
              "      <td>1988</td>\n",
              "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100-storing-covariance-by-the-associative-long...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000</td>\n",
              "      <td>1994</td>\n",
              "      <td>Bayesian Query Construction for Neural Network...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1000-bayesian-query-construction-for-neural-ne...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1001</td>\n",
              "      <td>1994</td>\n",
              "      <td>Neural Network Ensembles, Cross Validation, an...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1001-neural-network-ensembles-cross-validation...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3f51c9e2-c12c-4ed5-9e66-1f1d2f989b3d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3f51c9e2-c12c-4ed5-9e66-1f1d2f989b3d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3f51c9e2-c12c-4ed5-9e66-1f1d2f989b3d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     id  year                                              title event_type  \\\n",
              "0     1  1987  Self-Organization of Associative Database and ...        NaN   \n",
              "1    10  1987  A Mean Field Theory of Layer IV of Visual Cort...        NaN   \n",
              "2   100  1988  Storing Covariance by the Associative Long-Ter...        NaN   \n",
              "3  1000  1994  Bayesian Query Construction for Neural Network...        NaN   \n",
              "4  1001  1994  Neural Network Ensembles, Cross Validation, an...        NaN   \n",
              "\n",
              "                                            pdf_name          abstract  \\\n",
              "0  1-self-organization-of-associative-database-an...  Abstract Missing   \n",
              "1  10-a-mean-field-theory-of-layer-iv-of-visual-c...  Abstract Missing   \n",
              "2  100-storing-covariance-by-the-associative-long...  Abstract Missing   \n",
              "3  1000-bayesian-query-construction-for-neural-ne...  Abstract Missing   \n",
              "4  1001-neural-network-ensembles-cross-validation...  Abstract Missing   \n",
              "\n",
              "                                          paper_text  \n",
              "0  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n",
              "1  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n",
              "2  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...  \n",
              "3  Bayesian Query Construction for Neural\\nNetwor...  \n",
              "4  Neural Network Ensembles, Cross\\nValidation, a...  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['paper_text'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "XPgyNBADJSx2",
        "outputId": "fbfd9294-d09e-4f51-d86e-9f296b551323"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABASE\\nAND ITS APPLICATIONS\\nHisashi Suzuki and Suguru Arimoto\\nOsaka University, Toyonaka, Osaka 560, Japan\\nABSTRACT\\nAn efficient method of self-organizing associative databases is proposed together with\\napplications to robot eyesight systems. The proposed databases can associate any input\\nwith some output. In the first half part of discussion, an algorithm of self-organization is\\nproposed. From an aspect of hardware, it produces a new style of neural network. In the\\nlatter half part, an applicability to handwritten letter recognition and that to an autonomous\\nmobile robot system are demonstrated.\\n\\nINTRODUCTION\\nLet a mapping f : X -+ Y be given. Here, X is a finite or infinite set, and Y is another\\nfinite or infinite set. A learning machine observes any set of pairs (x, y) sampled randomly\\nfrom X x Y. (X x Y means the Cartesian product of X and Y.) And, it computes some\\nestimate j : X -+ Y of f to make small, the estimation error in some measure.\\nUsually we say that: the faster the decrease of estimation error with increase of the number of samples, the better the learning machine. However, such expression on performance\\nis incomplete. Since, it lacks consideration on the candidates of J of j assumed preliminarily. Then, how should we find out good learning machines? To clarify this conception,\\nlet us discuss for a while on some types of learning machines. And, let us advance the\\nunderstanding of the self-organization of associative database .\\n. Parameter Type\\nAn ordinary type of learning machine assumes an equation relating x\\'s and y\\'s with\\nparameters being indefinite, namely, a structure of f. It is equivalent to define implicitly a\\nset F of candidates of\\n(F is some subset of mappings from X to Y.) And, it computes\\nvalues of the parameters based on the observed samples. We call such type a parameter\\ntype.\\nFor a learning machine defined well, if F 3 f, j approaches f as the number of samples\\nincreases. In the alternative case, however, some estimation error remains eternally. Thus,\\na problem of designing a learning machine returns to find out a proper structure of f in this\\nsense.\\nOn the other hand, the assumed structure of f is demanded to be as compact as possible\\nto achieve a fast learning. In other words, the number of parameters should be small. Since,\\nif the parameters are few, some j can be uniquely determined even though the observed\\nsamples are few. However, this demand of being proper contradicts to that of being compact.\\nConsequently, in the parameter type, the better the compactness of the assumed structure\\nthat is proper, the better the learning machine. This is the most elementary conception\\nwhen we design learning machines .\\n\\n1.\\n\\n. Universality and Ordinary Neural Networks\\nNow suppose that a sufficient knowledge on f is given though J itself is unknown. In\\nthis case, it is comparatively easy to find out proper and compact structures of J. In the\\nalternative case, however, it is sometimes difficult. A possible solution is to give up the\\ncompactness and assume an almighty structure that can cover various 1\\'s. A combination\\nof some orthogonal bases of the infinite dimension is such a structure. Neural networks 1 ,2\\nare its approximations obtained by truncating finitely the dimension for implementation.\\n\\n? American Institute of Physics 1988\\n\\n\\x0c768\\nA main topic in designing neural networks is to establish such desirable structures of 1.\\nThis work includes developing practical procedures that compute values of coefficients from\\nthe observed samples. Such discussions are :flourishing since 1980 while many efficient methods have been proposed. Recently, even hardware units computing coefficients in parallel\\nfor speed-up are sold, e.g., ANZA, Mark III, Odyssey and E-1.\\nNevertheless, in neural networks, there always exists a danger of some error remaining\\neternally in estimating /. Precisely speaking, suppose that a combination of the bases of a\\nfinite number can define a structure of 1 essentially. In other words, suppose that F 3 /, or\\n1 is located near F. In such case, the estimation error is none or negligible. However, if 1\\nis distant from F, the estimation error never becomes negligible. Indeed, many researches\\nreport that the following situation appears when 1 is too complex. Once the estimation\\nerror converges to some value (> 0) as the number of samples increases, it decreases hardly\\neven though the dimension is heighten. This property sometimes is a considerable defect of\\nneural networks .\\n. Recursi ve Type\\nThe recursive type is founded on another methodology of learning that should be as\\nfollows. At the initial stage of no sample, the set Fa (instead of notation F) of candidates\\nof I equals to the set of all mappings from X to Y. After observing the first sample\\n(Xl, Yl) E X x Y, Fa is reduced to Fi so that I(xt) = Yl for any I E F. After observing\\nthe second sample (X2\\' Y2) E X x Y, Fl is further reduced to F2 so that i(xt) = Yl and\\nI(X2) = Y2 for any I E F. Thus, the candidate set F becomes gradually small as observation\\nof samples proceeds. The after observing i-samples, which we write\\nis one of the most\\nlikelihood estimation of 1 selected in fi;. Hence, contrarily to the parameter type, the\\nrecursive type guarantees surely that j approaches to 1 as the number of samples increases.\\nThe recursive type, if observes a sample (x\" yd, rewrites values 1,-l(X),S to I,(x)\\'s for\\nsome x\\'s correlated to the sample. Hence, this type has an architecture composed of a rule\\nfor rewriting and a free memory space. Such architecture forms naturally a kind of database\\nthat builds up management systems of data in a self-organizing way. However, this database\\ndiffers from ordinary ones in the following sense. It does not only record the samples already\\nobserved, but computes some estimation of l(x) for any x E X. We call such database an\\nassociative database.\\nThe first subject in constructing associative databases is how we establish the rule for\\nrewri ting. For this purpose, we adap t a measure called the dissimilari ty. Here, a dissimilari ty\\nmeans a mapping d : X x X -+ {reals > O} such that for any (x, x) E X x X, d(x, x) > 0\\nwhenever l(x) t /(x). However, it is not necessarily defined with a single formula. It is\\ndefinable with, for example, a collection of rules written in forms of \"if? .. then?? .. \"\\nThe dissimilarity d defines a structure of 1 locally in X x Y. Hence, even though\\nthe knowledge on f is imperfect, we can re:flect it on d in some heuristic way. Hence,\\ncontrarily to neural networks, it is possible to accelerate the speed of learning by establishing\\nd well. Especially, we can easily find out simple d\\'s for those l\\'s which process analogically\\ninformation like a human. (See the applications in this paper.) And, for such /\\'s, the\\nrecursive type shows strongly its effectiveness.\\nWe denote a sequence of observed samples by (Xl, Yd, (X2\\' Y2),???. One of the simplest\\nconstructions of associative databases after observing i-samples (i = 1,2,.,,) is as follows.\\n\\ni\\n\\ni\"\\n\\nI,\\n\\nAlgorithm 1. At the initial stage, let So be the empty set. For every i =\\n1,2\" .. , let i,-l(x) for any x E X equal some y* such that (x*,y*) E S,-l and\\n\\nd(x, x*) =\\n\\nmin\\n(%,y)ES.-t\\n\\nd(x, x) .\\n\\nFurthermore, add (x\" y,) to S;-l to produce Sa, i.e., S, = S,_l U {(x\"\\n\\n(1)\\n\\ny,n.\\n\\n\\x0c769\\n\\nAnother version improved to economize the memory is as follows.\\n\\nAlgorithm 2, At the initial stage, let So be composed of an arbitrary element\\nin X x Y. For every i = 1,2\"\", let ii-lex) for any x E X equal some y. such\\nthat (x?, y.) E Si-l and\\nd(x, x?) =\\n\\nmin\\n\\nd(x, x) .\\n\\n(i,i)ES.-l\\n\\nFurthermore, if ii-l(Xi) # Yi then let Si = Si-l, or add (Xi, Yi) to Si-l to\\nproduce Si, i.e., Si = Si-l U {(Xi, Yi)}\\'\\nIn either construction, ii approaches to f as i increases. However, the computation time\\ngrows proportionally to the size of Si. The second subject in constructing associative\\ndatabases is what addressing rule we should employ to economize the computation time. In\\nthe subsequent chapters, a construction of associative database for this purpose is proposed.\\nIt manages data in a form of binary tree.\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABASE\\nGiven a sample sequence (Xl, Yl), (X2\\' Y2), .. \" the algorithm for constructing associative\\ndatabase is as follows.\\n\\nAlgorithm 3,\\'\\n\\nStep I(Initialization): Let (x[root], y[root]) = (Xl, Yd. Here, x[.] and y[.] are\\nvariables assigned for respective nodes to memorize data.. Furthermore, let t = 1.\\nStep 2: Increase t by 1, and put x, in. After reset a pointer n to the root, repeat\\nthe following until n arrives at some terminal node, i.e., leaf.\\nNotations nand\\nd(xt, x[n)), let n\\n\\nn mean the descendant nodes of n.\\n=n. Otherwise, let n =n.\\n\\nIf d(x\" r[n)) ~\\n\\nStep 3: Display yIn] as the related information. Next, put y, in. If yIn] = y\" back\\nto step 2. Otherwise, first establish new descendant nodes n and n. Secondly,\\nlet\\n\\n(x[n], yIn))\\n(x[n], yIn))\\n\\n(x[n], yIn)),\\n(Xt, y,).\\n\\n(2)\\n(3)\\n\\nFinally, back to step 2. Here, the loop of step 2-3 can be stopped at any time\\nand also can be continued.\\nNow, suppose that gate elements, namely, artificial \"synapses\" that play the role of branching by d are prepared. Then, we obtain a new style of neural network with gate elements\\nbeing randomly connected by this algorithm.\\n\\nLETTER RECOGNITION\\nRecen tly, the vertical slitting method for recognizing typographic English letters3 , the\\nelastic matching method for recognizing hand written discrete English letters4 , the global\\ntraining and fuzzy logic search method for recognizing Chinese characters written in square\\nstyleS, etc. are published. The self-organization of associative database realizes the recognition of handwritten continuous English letters.\\n\\n\\x0c770\\n\\n9 /wn\"\\n\\nNOV\\n\\n~ ~ ~ -xk :La.t\\n\\n~~ ~ ~~~\\n\\ndw1lo\\'\\n\\n~~~~~of~~\\n\\n~~~ 4,-?~~4Fig. 1. Source document.\\n2~~---------------\\'\\n\\nlOO~---------------\\'\\n\\nH\\n\\no\\n\\no\\nFig. 2. Windowing.\\n\\n1000\\n\\n2000\\n\\n3000\\n\\n4000\\n\\nNumber of samples\\n\\no\\n\\n1000\\n\\n2000\\n\\n3000\\n\\n4000\\n\\nNUAlber of sampl es\\n\\nFig. 3. An experiment result.\\n\\nAn image scanner takes a document image (Fig. 1). The letter recognizer uses a parallelogram window that at least can cover the maximal letter (Fig. 2), and processes the\\nsequence of letters while shifting the window. That is, the recognizer scans a word in a\\nslant direction. And, it places the window so that its left vicinity may be on the first black\\npoint detected. Then, the window catches a letter and some part of the succeeding letter.\\nIf recognition of the head letter is performed, its end position, namely, the boundary line\\nbetween two letters becomes known. Hence, by starting the scanning from this boundary\\nand repeating the above operations, the recognizer accomplishes recursively the task. Thus\\nthe major problem comes to identifying the head letter in the window.\\nConsidering it, we define the following.\\n? Regard window images as x\\'s, and define X accordingly.\\n? For a (x, x) E X x X, denote by B a black point in the left area from the boundary on\\nwindow image X. Project each B onto window image x. Then, measure the Euclidean\\ndistance 6 between fj and a black point B on x being the closest to B. Let d(x, x) be\\nthe summation of 6\\'s for all black points B\\'s on x divided by the number of B\\'s.\\n? Regard couples of the \"reading\" and the position of boundary as y\\'s, and define Y\\naccordingly.\\nAn operator teaches the recognizer in interaction the relation between window image and\\nreading& boundary with algorithm 3. Precisely, if the recalled reading is incorrect, the\\noperator teaches a correct reading via the console. Moreover, if the boundary position is\\nincorrect, he teaches a correct position via the mouse.\\nFig. 1 shows partially a document image used in this experiment. Fig. 3 shows the\\nchange of the number of nodes and that of the recognition rate defined as the relative\\nfrequency of correct answers in the past 1000 trials. Speciiications of the window are height\\n= 20dot, width = 10dot, and slant angular = 68deg. In this example, the levels of tree\\nwere distributed in 6-19 at time 4000 and the recognition rate converged to about 74%.\\nExperimentally, the recognition rate converges to about 60-85% in most cases, and to 95% at\\na rare case. However, it does not attain 100% since, e.g., \"c\" and \"e\" are not distinguishable\\nbecause of excessive lluctuation in writing. If the consistency of the x, y-relation is not\\nassured like this, the number of nodes increases endlessly (d. Fig. 3). Hence, it is clever to\\nstop the learning when the recognition rate attains some upper limit. To improve further\\nthe recognition rate, we must consider the spelling of words. It is one of future subjects.\\n\\n\\x0c771\\n\\nOBSTACLE AVOIDING MOVEMENT\\nVarious systems of camera type autonomous mobile robot are reported flourishingly6-1O.\\nThe system made up by the authors (Fig. 4) also belongs to this category. Now, in mathematical methodologies, we solve usually the problem of obstacle avoiding movement as\\na cost minimization problem under some cost criterion established artificially. Contrarily,\\nthe self-organization of associative database reproduces faithfully the cost criterion of an\\noperator. Therefore, motion of the robot after learning becomes very natural.\\nNow, the length, width and height of the robot are all about O.7m, and the weight is\\nabout 30kg. The visual angle of camera is about 55deg. The robot has the following three\\nfactors of motion. It turns less than ?30deg, advances less than 1m, and controls speed less\\nthan 3km/h. The experiment was done on the passageway of wid th 2.5m inside a building\\nwhich the authors\\' laboratories exist in (Fig. 5). Because of an experimental intention, we\\narrange boxes, smoking stands, gas cylinders, stools, handcarts, etc. on the passage way at\\nrandom. We let the robot take an image through the camera, recall a similar image, and\\ntrace the route preliminarily recorded on it. For this purpose, we define the following.\\n? Let the camera face 28deg downward to take an image, and process it through a low\\npass filter. Scanning vertically the filtered image from the bottom to the top, search\\nthe first point C where the luminance changes excessively. Then, su bstitu te all points\\nfrom the bottom to C for white, and all points from C to the top for black (Fig. 6).\\n(If no obstacle exists just in front of the robot, the white area shows the \\'\\'free\\'\\' area\\nwhere the robot can move around.) Regard binary 32 x 32dot images processed thus\\nas x\\'s, and define X accordingly.\\n? For every (x, x) E X x X, let d(x, x) be the number of black points on the exclusive-or\\nimage between x and X.\\n? Regard as y\\'s the images obtained by drawing routes on images x\\'s, and define Y\\naccordingly.\\nThe robot superimposes, on the current camera image x, the route recalled for x, and\\ninquires the operator instructions. The operator judges subjectively whether the suggested\\nroute is appropriate or not. In the negative answer, he draws a desirable route on x with the\\nmouse to teach a new y to the robot. This opera.tion defines implicitly a sample sequence\\nof (x, y) reflecting the cost criterion of the operator.\\n\\n.::l\" !\\n-\\n\\nIibUBe\\n\\n_. -\\n\\n22\\n\\n11\\n\\nRoan\\n\\n12\\n\\n{-\\n\\n13\\n\\nStationary uni t\\n\\nFig. 4. Configuration of\\nautonomous mobile robot system.\\n\\n~\\n\\nI\\n\\n,\\n\\n23\\n\\n24\\n\\nNorth\\n14\\n\\nrmbi Ie unit (robot)\\n\\n-\\n\\nRoan\\n\\ny\\n\\nt\\n\\nFig. 5. Experimental\\nenvironment.\\n\\n\\x0c772\\n\\nWall\\n\\nCamera image\\n\\nPreprocessing\\n\\nA\\n\\n::: !fa\\n\\n?\\n\\nPreprocessing\\n\\n0\\n\\nO\\n\\nCourse\\nsuggest ion\\n\\n??\\n\\n..\\n\\nSearch\\n\\nA\\n\\nFig. 6. Processing for\\nobstacle avoiding movement.\\n\\nx\\n\\nFig. 1. Processing for\\nposition identification.\\nWe define the satisfaction rate by the relative frequency of acceptable suggestions of\\nroute in the past 100 trials. In a typical experiment, the change of satisfaction rate showed\\na similar tendency to Fig. 3, and it attains about 95% around time 800. Here, notice that\\nthe rest 5% does not mean directly the percentage of collision. (In practice, we prevent the\\ncollision by adopting some supplementary measure.) At time 800, the number of nodes was\\n145, and the levels of tree were distributed in 6-17.\\nThe proposed method reflects delicately various characters of operator. For example, a\\nrobot trained by an operator 0 moves slowly with enough space against obstacles while one\\ntrained by another operator 0\\' brushes quickly against obstacles. This fact gives us a hint\\non a method of printing \"characters\" into machines.\\nPOSITION IDENTIFICATION\\nThe robot can identify its position by recalling a similar landscape with the position data\\nto a camera image. For this purpose, in principle, it suffices to regard camera images and\\nposition data as x\\'s and y\\'s, respectively. However, the memory capacity is finite in actual\\ncompu ters. Hence, we cannot but compress the camera images at a slight loss of information.\\nSuch compression is admittable as long as the precision of position identification is in an\\nacceptable area. Thus, the major problem comes to find out some suitable compression\\nmethod.\\nIn the experimental environment (Fig. 5), juts are on the passageway at intervals of\\n3.6m, and each section between adjacent juts has at most one door. The robot identifies\\nroughly from a surrounding landscape which section itself places in. And, it uses temporarily\\na triangular surveying technique if an exact measure is necessary. To realize the former task,\\nwe define the following .\\n? Turn the camera to take a panorama image of 360deg. Scanning horizontally the\\ncenter line, substitute the points where the luminance excessively changes for black\\nand the other points for white (Fig. 1). Regard binary 360dot line images processed\\nthus as x\\'s, and define X accordingly.\\n? For every (x, x) E X x X, project each black point A on x onto x. And, measure the\\nEuclidean distance 6 between A and a black point A on x being the closest to A. Let\\nthe summation of 6 be S. Similarly, calculate S by exchanging the roles of x and X.\\nDenoting the numbers of A\\'s and A\\'s respectively by nand n, define\\n\\n\\x0c773\\n\\nd(x, x) =\\n\\n~(~\\n+ ~).\\n2 n\\nn\\n\\n(4)\\n\\n? Regard positive integers labeled on sections as y\\'s (cf. Fig. 5), and define Y accordingly.\\nIn the learning mode, the robot checks exactly its position with a counter that is reset periodically by the operator. The robot runs arbitrarily on the passageways within 18m area\\nand learns the relation between landscapes and position data. (Position identification beyond 18m area is achieved by crossing plural databases one another.) This task is automatic\\nexcepting the periodic reset of counter, namely, it is a kind of learning without teacher.\\nWe define the identification rate by the relative frequency of correct recalls of position\\ndata in the past 100 trials. In a typical example, it converged to about 83% around time\\n400. At time 400, the number of levels was 202, and the levels oftree were distributed in 522. Since the identification failures of 17% can be rejected by considering the trajectory, no\\npro blem arises in practical use. In order to improve the identification rate, the compression\\nratio of camera images must be loosened. Such possibility depends on improvement of the\\nhardware in the future.\\nFig. 8 shows an example of actual motion of the robot based on the database for obstacle\\navoiding movement and that for position identification. This example corresponds to a case\\nof moving from 14 to 23 in Fig. 5. Here, the time interval per frame is about 40sec.\\n\\n,~. .~ (\\n;~\"i..\\n~\\n\\n\"\\n\\n\"\\n\\n.\\n\\n..I\\n\\nI\\n\\n?\\n?\\n\\n\"\\n\\nI\\'\\n.\\n\\'.1\\nt\\n\\n;\\n\\ni\\n\\n-:\\n, . . , \\'II\\n\\nFig. 8. Actual motion of the robot.\\n\\n\\x0c774\\n\\nCONCLUSION\\nA method of self-organizing associative databases was proposed with the application to\\nrobot eyesight systems. The machine decomposes a global structure unknown into a set of\\nlocal structures known and learns universally any input-output response. This framework\\nof problem implies a wide application area other than the examples shown in this paper.\\nA defect of the algorithm 3 of self-organization is that the tree is balanced well only\\nfor a subclass of structures of f. A subject imposed us is to widen the class. A probable\\nsolution is to abolish the addressing rule depending directly on values of d and, instead, to\\nestablish another rule depending on the distribution function of values of d. It is now under\\ninvestigation.\\n\\nREFERENCES\\n1. Hopfield, J. J. and D. W. Tank, \"Computing with Neural Circuit: A Model/\\'\\n\\nScience 233 (1986), pp. 625-633.\\n2. Rumelhart, D. E. et al., \"Learning Representations by Back-Propagating Errors,\" Nature 323 (1986), pp. 533-536.\\n\\n3. Hull, J. J., \"Hypothesis Generation in a Computational Model for Visual Word\\nRecognition,\" IEEE Expert, Fall (1986), pp. 63-70.\\n4. Kurtzberg, J. M., \"Feature Analysis for Symbol Recognition by Elastic Matching,\" IBM J. Res. Develop. 31-1 (1987), pp. 91-95.\\n\\n5. Wang, Q. R. and C. Y. Suen, \"Large Tree Classifier with Heuristic Search and\\nGlobal Training,\" IEEE Trans. Pattern. Anal. & Mach. Intell. PAMI 9-1\\n(1987) pp. 91-102.\\n6. Brooks, R. A. et al, \"Self Calibration of Motion and Stereo Vision for Mobile\\nRobots,\" 4th Int. Symp. of Robotics Research (1987), pp. 267-276.\\n7. Goto, Y. and A. Stentz, \"The CMU System for Mobile Robot Navigation,\" 1987\\nIEEE Int. Conf. on Robotics & Automation (1987), pp. 99-105.\\n8. Madarasz, R. et al., \"The Design of an Autonomous Vehicle for the Disabled,\"\\nIEEE Jour. of Robotics & Automation RA 2-3 (1986), pp. 117-125.\\n9. Triendl, E. and D. J. Kriegman, \"Stereo Vision and Navigation within Buildings,\" 1987 IEEE Int. Conf. on Robotics & Automation (1987), pp. 1725-1730.\\n10. Turk, M. A. et al., \"Video Road-Following for the Autonomous Land Vehicle,\"\\n1987 IEEE Int. Conf. on Robotics & Automation (1987), pp. 273-279.\\n\\n\\x0c'"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['cleaned_paper_text']=df['paper_text'].str.replace('\\n',' ')"
      ],
      "metadata": {
        "id": "uKN55odUJUSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['cleaned_paper_text'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "0I05RigkLvYK",
        "outputId": "2548accf-7c74-4dde-c826-7055f6d9a3e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'767  SELF-ORGANIZATION OF ASSOCIATIVE DATABASE AND ITS APPLICATIONS Hisashi Suzuki and Suguru Arimoto Osaka University, Toyonaka, Osaka 560, Japan ABSTRACT An efficient method of self-organizing associative databases is proposed together with applications to robot eyesight systems. The proposed databases can associate any input with some output. In the first half part of discussion, an algorithm of self-organization is proposed. From an aspect of hardware, it produces a new style of neural network. In the latter half part, an applicability to handwritten letter recognition and that to an autonomous mobile robot system are demonstrated.  INTRODUCTION Let a mapping f : X -+ Y be given. Here, X is a finite or infinite set, and Y is another finite or infinite set. A learning machine observes any set of pairs (x, y) sampled randomly from X x Y. (X x Y means the Cartesian product of X and Y.) And, it computes some estimate j : X -+ Y of f to make small, the estimation error in some measure. Usually we say that: the faster the decrease of estimation error with increase of the number of samples, the better the learning machine. However, such expression on performance is incomplete. Since, it lacks consideration on the candidates of J of j assumed preliminarily. Then, how should we find out good learning machines? To clarify this conception, let us discuss for a while on some types of learning machines. And, let us advance the understanding of the self-organization of associative database . . Parameter Type An ordinary type of learning machine assumes an equation relating x\\'s and y\\'s with parameters being indefinite, namely, a structure of f. It is equivalent to define implicitly a set F of candidates of (F is some subset of mappings from X to Y.) And, it computes values of the parameters based on the observed samples. We call such type a parameter type. For a learning machine defined well, if F 3 f, j approaches f as the number of samples increases. In the alternative case, however, some estimation error remains eternally. Thus, a problem of designing a learning machine returns to find out a proper structure of f in this sense. On the other hand, the assumed structure of f is demanded to be as compact as possible to achieve a fast learning. In other words, the number of parameters should be small. Since, if the parameters are few, some j can be uniquely determined even though the observed samples are few. However, this demand of being proper contradicts to that of being compact. Consequently, in the parameter type, the better the compactness of the assumed structure that is proper, the better the learning machine. This is the most elementary conception when we design learning machines .  1.  . Universality and Ordinary Neural Networks Now suppose that a sufficient knowledge on f is given though J itself is unknown. In this case, it is comparatively easy to find out proper and compact structures of J. In the alternative case, however, it is sometimes difficult. A possible solution is to give up the compactness and assume an almighty structure that can cover various 1\\'s. A combination of some orthogonal bases of the infinite dimension is such a structure. Neural networks 1 ,2 are its approximations obtained by truncating finitely the dimension for implementation.  ? American Institute of Physics 1988  \\x0c768 A main topic in designing neural networks is to establish such desirable structures of 1. This work includes developing practical procedures that compute values of coefficients from the observed samples. Such discussions are :flourishing since 1980 while many efficient methods have been proposed. Recently, even hardware units computing coefficients in parallel for speed-up are sold, e.g., ANZA, Mark III, Odyssey and E-1. Nevertheless, in neural networks, there always exists a danger of some error remaining eternally in estimating /. Precisely speaking, suppose that a combination of the bases of a finite number can define a structure of 1 essentially. In other words, suppose that F 3 /, or 1 is located near F. In such case, the estimation error is none or negligible. However, if 1 is distant from F, the estimation error never becomes negligible. Indeed, many researches report that the following situation appears when 1 is too complex. Once the estimation error converges to some value (> 0) as the number of samples increases, it decreases hardly even though the dimension is heighten. This property sometimes is a considerable defect of neural networks . . Recursi ve Type The recursive type is founded on another methodology of learning that should be as follows. At the initial stage of no sample, the set Fa (instead of notation F) of candidates of I equals to the set of all mappings from X to Y. After observing the first sample (Xl, Yl) E X x Y, Fa is reduced to Fi so that I(xt) = Yl for any I E F. After observing the second sample (X2\\' Y2) E X x Y, Fl is further reduced to F2 so that i(xt) = Yl and I(X2) = Y2 for any I E F. Thus, the candidate set F becomes gradually small as observation of samples proceeds. The after observing i-samples, which we write is one of the most likelihood estimation of 1 selected in fi;. Hence, contrarily to the parameter type, the recursive type guarantees surely that j approaches to 1 as the number of samples increases. The recursive type, if observes a sample (x\" yd, rewrites values 1,-l(X),S to I,(x)\\'s for some x\\'s correlated to the sample. Hence, this type has an architecture composed of a rule for rewriting and a free memory space. Such architecture forms naturally a kind of database that builds up management systems of data in a self-organizing way. However, this database differs from ordinary ones in the following sense. It does not only record the samples already observed, but computes some estimation of l(x) for any x E X. We call such database an associative database. The first subject in constructing associative databases is how we establish the rule for rewri ting. For this purpose, we adap t a measure called the dissimilari ty. Here, a dissimilari ty means a mapping d : X x X -+ {reals > O} such that for any (x, x) E X x X, d(x, x) > 0 whenever l(x) t /(x). However, it is not necessarily defined with a single formula. It is definable with, for example, a collection of rules written in forms of \"if? .. then?? .. \" The dissimilarity d defines a structure of 1 locally in X x Y. Hence, even though the knowledge on f is imperfect, we can re:flect it on d in some heuristic way. Hence, contrarily to neural networks, it is possible to accelerate the speed of learning by establishing d well. Especially, we can easily find out simple d\\'s for those l\\'s which process analogically information like a human. (See the applications in this paper.) And, for such /\\'s, the recursive type shows strongly its effectiveness. We denote a sequence of observed samples by (Xl, Yd, (X2\\' Y2),???. One of the simplest constructions of associative databases after observing i-samples (i = 1,2,.,,) is as follows.  i  i\"  I,  Algorithm 1. At the initial stage, let So be the empty set. For every i = 1,2\" .. , let i,-l(x) for any x E X equal some y* such that (x*,y*) E S,-l and  d(x, x*) =  min (%,y)ES.-t  d(x, x) .  Furthermore, add (x\" y,) to S;-l to produce Sa, i.e., S, = S,_l U {(x\"  (1)  y,n.  \\x0c769  Another version improved to economize the memory is as follows.  Algorithm 2, At the initial stage, let So be composed of an arbitrary element in X x Y. For every i = 1,2\"\", let ii-lex) for any x E X equal some y. such that (x?, y.) E Si-l and d(x, x?) =  min  d(x, x) .  (i,i)ES.-l  Furthermore, if ii-l(Xi) # Yi then let Si = Si-l, or add (Xi, Yi) to Si-l to produce Si, i.e., Si = Si-l U {(Xi, Yi)}\\' In either construction, ii approaches to f as i increases. However, the computation time grows proportionally to the size of Si. The second subject in constructing associative databases is what addressing rule we should employ to economize the computation time. In the subsequent chapters, a construction of associative database for this purpose is proposed. It manages data in a form of binary tree.  SELF-ORGANIZATION OF ASSOCIATIVE DATABASE Given a sample sequence (Xl, Yl), (X2\\' Y2), .. \" the algorithm for constructing associative database is as follows.  Algorithm 3,\\'  Step I(Initialization): Let (x[root], y[root]) = (Xl, Yd. Here, x[.] and y[.] are variables assigned for respective nodes to memorize data.. Furthermore, let t = 1. Step 2: Increase t by 1, and put x, in. After reset a pointer n to the root, repeat the following until n arrives at some terminal node, i.e., leaf. Notations nand d(xt, x[n)), let n  n mean the descendant nodes of n. =n. Otherwise, let n =n.  If d(x\" r[n)) ~  Step 3: Display yIn] as the related information. Next, put y, in. If yIn] = y\" back to step 2. Otherwise, first establish new descendant nodes n and n. Secondly, let  (x[n], yIn)) (x[n], yIn))  (x[n], yIn)), (Xt, y,).  (2) (3)  Finally, back to step 2. Here, the loop of step 2-3 can be stopped at any time and also can be continued. Now, suppose that gate elements, namely, artificial \"synapses\" that play the role of branching by d are prepared. Then, we obtain a new style of neural network with gate elements being randomly connected by this algorithm.  LETTER RECOGNITION Recen tly, the vertical slitting method for recognizing typographic English letters3 , the elastic matching method for recognizing hand written discrete English letters4 , the global training and fuzzy logic search method for recognizing Chinese characters written in square styleS, etc. are published. The self-organization of associative database realizes the recognition of handwritten continuous English letters.  \\x0c770  9 /wn\"  NOV  ~ ~ ~ -xk :La.t  ~~ ~ ~~~  dw1lo\\'  ~~~~~of~~  ~~~ 4,-?~~4Fig. 1. Source document. 2~~---------------\\'  lOO~---------------\\'  H  o  o Fig. 2. Windowing.  1000  2000  3000  4000  Number of samples  o  1000  2000  3000  4000  NUAlber of sampl es  Fig. 3. An experiment result.  An image scanner takes a document image (Fig. 1). The letter recognizer uses a parallelogram window that at least can cover the maximal letter (Fig. 2), and processes the sequence of letters while shifting the window. That is, the recognizer scans a word in a slant direction. And, it places the window so that its left vicinity may be on the first black point detected. Then, the window catches a letter and some part of the succeeding letter. If recognition of the head letter is performed, its end position, namely, the boundary line between two letters becomes known. Hence, by starting the scanning from this boundary and repeating the above operations, the recognizer accomplishes recursively the task. Thus the major problem comes to identifying the head letter in the window. Considering it, we define the following. ? Regard window images as x\\'s, and define X accordingly. ? For a (x, x) E X x X, denote by B a black point in the left area from the boundary on window image X. Project each B onto window image x. Then, measure the Euclidean distance 6 between fj and a black point B on x being the closest to B. Let d(x, x) be the summation of 6\\'s for all black points B\\'s on x divided by the number of B\\'s. ? Regard couples of the \"reading\" and the position of boundary as y\\'s, and define Y accordingly. An operator teaches the recognizer in interaction the relation between window image and reading& boundary with algorithm 3. Precisely, if the recalled reading is incorrect, the operator teaches a correct reading via the console. Moreover, if the boundary position is incorrect, he teaches a correct position via the mouse. Fig. 1 shows partially a document image used in this experiment. Fig. 3 shows the change of the number of nodes and that of the recognition rate defined as the relative frequency of correct answers in the past 1000 trials. Speciiications of the window are height = 20dot, width = 10dot, and slant angular = 68deg. In this example, the levels of tree were distributed in 6-19 at time 4000 and the recognition rate converged to about 74%. Experimentally, the recognition rate converges to about 60-85% in most cases, and to 95% at a rare case. However, it does not attain 100% since, e.g., \"c\" and \"e\" are not distinguishable because of excessive lluctuation in writing. If the consistency of the x, y-relation is not assured like this, the number of nodes increases endlessly (d. Fig. 3). Hence, it is clever to stop the learning when the recognition rate attains some upper limit. To improve further the recognition rate, we must consider the spelling of words. It is one of future subjects.  \\x0c771  OBSTACLE AVOIDING MOVEMENT Various systems of camera type autonomous mobile robot are reported flourishingly6-1O. The system made up by the authors (Fig. 4) also belongs to this category. Now, in mathematical methodologies, we solve usually the problem of obstacle avoiding movement as a cost minimization problem under some cost criterion established artificially. Contrarily, the self-organization of associative database reproduces faithfully the cost criterion of an operator. Therefore, motion of the robot after learning becomes very natural. Now, the length, width and height of the robot are all about O.7m, and the weight is about 30kg. The visual angle of camera is about 55deg. The robot has the following three factors of motion. It turns less than ?30deg, advances less than 1m, and controls speed less than 3km/h. The experiment was done on the passageway of wid th 2.5m inside a building which the authors\\' laboratories exist in (Fig. 5). Because of an experimental intention, we arrange boxes, smoking stands, gas cylinders, stools, handcarts, etc. on the passage way at random. We let the robot take an image through the camera, recall a similar image, and trace the route preliminarily recorded on it. For this purpose, we define the following. ? Let the camera face 28deg downward to take an image, and process it through a low pass filter. Scanning vertically the filtered image from the bottom to the top, search the first point C where the luminance changes excessively. Then, su bstitu te all points from the bottom to C for white, and all points from C to the top for black (Fig. 6). (If no obstacle exists just in front of the robot, the white area shows the \\'\\'free\\'\\' area where the robot can move around.) Regard binary 32 x 32dot images processed thus as x\\'s, and define X accordingly. ? For every (x, x) E X x X, let d(x, x) be the number of black points on the exclusive-or image between x and X. ? Regard as y\\'s the images obtained by drawing routes on images x\\'s, and define Y accordingly. The robot superimposes, on the current camera image x, the route recalled for x, and inquires the operator instructions. The operator judges subjectively whether the suggested route is appropriate or not. In the negative answer, he draws a desirable route on x with the mouse to teach a new y to the robot. This opera.tion defines implicitly a sample sequence of (x, y) reflecting the cost criterion of the operator.  .::l\" ! -  IibUBe  _. -  22  11  Roan  12  {-  13  Stationary uni t  Fig. 4. Configuration of autonomous mobile robot system.  ~  I  ,  23  24  North 14  rmbi Ie unit (robot)  -  Roan  y  t  Fig. 5. Experimental environment.  \\x0c772  Wall  Camera image  Preprocessing  A  ::: !fa  ?  Preprocessing  0  O  Course suggest ion  ??  ..  Search  A  Fig. 6. Processing for obstacle avoiding movement.  x  Fig. 1. Processing for position identification. We define the satisfaction rate by the relative frequency of acceptable suggestions of route in the past 100 trials. In a typical experiment, the change of satisfaction rate showed a similar tendency to Fig. 3, and it attains about 95% around time 800. Here, notice that the rest 5% does not mean directly the percentage of collision. (In practice, we prevent the collision by adopting some supplementary measure.) At time 800, the number of nodes was 145, and the levels of tree were distributed in 6-17. The proposed method reflects delicately various characters of operator. For example, a robot trained by an operator 0 moves slowly with enough space against obstacles while one trained by another operator 0\\' brushes quickly against obstacles. This fact gives us a hint on a method of printing \"characters\" into machines. POSITION IDENTIFICATION The robot can identify its position by recalling a similar landscape with the position data to a camera image. For this purpose, in principle, it suffices to regard camera images and position data as x\\'s and y\\'s, respectively. However, the memory capacity is finite in actual compu ters. Hence, we cannot but compress the camera images at a slight loss of information. Such compression is admittable as long as the precision of position identification is in an acceptable area. Thus, the major problem comes to find out some suitable compression method. In the experimental environment (Fig. 5), juts are on the passageway at intervals of 3.6m, and each section between adjacent juts has at most one door. The robot identifies roughly from a surrounding landscape which section itself places in. And, it uses temporarily a triangular surveying technique if an exact measure is necessary. To realize the former task, we define the following . ? Turn the camera to take a panorama image of 360deg. Scanning horizontally the center line, substitute the points where the luminance excessively changes for black and the other points for white (Fig. 1). Regard binary 360dot line images processed thus as x\\'s, and define X accordingly. ? For every (x, x) E X x X, project each black point A on x onto x. And, measure the Euclidean distance 6 between A and a black point A on x being the closest to A. Let the summation of 6 be S. Similarly, calculate S by exchanging the roles of x and X. Denoting the numbers of A\\'s and A\\'s respectively by nand n, define  \\x0c773  d(x, x) =  ~(~ + ~). 2 n n  (4)  ? Regard positive integers labeled on sections as y\\'s (cf. Fig. 5), and define Y accordingly. In the learning mode, the robot checks exactly its position with a counter that is reset periodically by the operator. The robot runs arbitrarily on the passageways within 18m area and learns the relation between landscapes and position data. (Position identification beyond 18m area is achieved by crossing plural databases one another.) This task is automatic excepting the periodic reset of counter, namely, it is a kind of learning without teacher. We define the identification rate by the relative frequency of correct recalls of position data in the past 100 trials. In a typical example, it converged to about 83% around time 400. At time 400, the number of levels was 202, and the levels oftree were distributed in 522. Since the identification failures of 17% can be rejected by considering the trajectory, no pro blem arises in practical use. In order to improve the identification rate, the compression ratio of camera images must be loosened. Such possibility depends on improvement of the hardware in the future. Fig. 8 shows an example of actual motion of the robot based on the database for obstacle avoiding movement and that for position identification. This example corresponds to a case of moving from 14 to 23 in Fig. 5. Here, the time interval per frame is about 40sec.  ,~. .~ ( ;~\"i.. ~  \"  \"  .  ..I  I  ? ?  \"  I\\' . \\'.1 t  ;  i  -: , . . , \\'II  Fig. 8. Actual motion of the robot.  \\x0c774  CONCLUSION A method of self-organizing associative databases was proposed with the application to robot eyesight systems. The machine decomposes a global structure unknown into a set of local structures known and learns universally any input-output response. This framework of problem implies a wide application area other than the examples shown in this paper. A defect of the algorithm 3 of self-organization is that the tree is balanced well only for a subclass of structures of f. A subject imposed us is to widen the class. A probable solution is to abolish the addressing rule depending directly on values of d and, instead, to establish another rule depending on the distribution function of values of d. It is now under investigation.  REFERENCES 1. Hopfield, J. J. and D. W. Tank, \"Computing with Neural Circuit: A Model/\\'  Science 233 (1986), pp. 625-633. 2. Rumelhart, D. E. et al., \"Learning Representations by Back-Propagating Errors,\" Nature 323 (1986), pp. 533-536.  3. Hull, J. J., \"Hypothesis Generation in a Computational Model for Visual Word Recognition,\" IEEE Expert, Fall (1986), pp. 63-70. 4. Kurtzberg, J. M., \"Feature Analysis for Symbol Recognition by Elastic Matching,\" IBM J. Res. Develop. 31-1 (1987), pp. 91-95.  5. Wang, Q. R. and C. Y. Suen, \"Large Tree Classifier with Heuristic Search and Global Training,\" IEEE Trans. Pattern. Anal. & Mach. Intell. PAMI 9-1 (1987) pp. 91-102. 6. Brooks, R. A. et al, \"Self Calibration of Motion and Stereo Vision for Mobile Robots,\" 4th Int. Symp. of Robotics Research (1987), pp. 267-276. 7. Goto, Y. and A. Stentz, \"The CMU System for Mobile Robot Navigation,\" 1987 IEEE Int. Conf. on Robotics & Automation (1987), pp. 99-105. 8. Madarasz, R. et al., \"The Design of an Autonomous Vehicle for the Disabled,\" IEEE Jour. of Robotics & Automation RA 2-3 (1986), pp. 117-125. 9. Triendl, E. and D. J. Kriegman, \"Stereo Vision and Navigation within Buildings,\" 1987 IEEE Int. Conf. on Robotics & Automation (1987), pp. 1725-1730. 10. Turk, M. A. et al., \"Video Road-Following for the Autonomous Land Vehicle,\" 1987 IEEE Int. Conf. on Robotics & Automation (1987), pp. 273-279.  \\x0c'"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "df['cleaned_paper_text']=df['cleaned_paper_text'].apply(lambda x : re.sub('[^a-zA-Z#\\s]*','',x))"
      ],
      "metadata": {
        "id": "ThHMKWrWMDSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['cleaned_paper_text'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "bPYrqOmhNqUN",
        "outputId": "1da05c31-7136-4a34-d863-142da582e426"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'  SELFORGANIZATION OF ASSOCIATIVE DATABASE AND ITS APPLICATIONS Hisashi Suzuki and Suguru Arimoto Osaka University Toyonaka Osaka  Japan ABSTRACT An efficient method of selforganizing associative databases is proposed together with applications to robot eyesight systems The proposed databases can associate any input with some output In the first half part of discussion an algorithm of selforganization is proposed From an aspect of hardware it produces a new style of neural network In the latter half part an applicability to handwritten letter recognition and that to an autonomous mobile robot system are demonstrated  INTRODUCTION Let a mapping f  X  Y be given Here X is a finite or infinite set and Y is another finite or infinite set A learning machine observes any set of pairs x y sampled randomly from X x Y X x Y means the Cartesian product of X and Y And it computes some estimate j  X  Y of f to make small the estimation error in some measure Usually we say that the faster the decrease of estimation error with increase of the number of samples the better the learning machine However such expression on performance is incomplete Since it lacks consideration on the candidates of J of j assumed preliminarily Then how should we find out good learning machines To clarify this conception let us discuss for a while on some types of learning machines And let us advance the understanding of the selforganization of associative database   Parameter Type An ordinary type of learning machine assumes an equation relating xs and ys with parameters being indefinite namely a structure of f It is equivalent to define implicitly a set F of candidates of F is some subset of mappings from X to Y And it computes values of the parameters based on the observed samples We call such type a parameter type For a learning machine defined well if F  f j approaches f as the number of samples increases In the alternative case however some estimation error remains eternally Thus a problem of designing a learning machine returns to find out a proper structure of f in this sense On the other hand the assumed structure of f is demanded to be as compact as possible to achieve a fast learning In other words the number of parameters should be small Since if the parameters are few some j can be uniquely determined even though the observed samples are few However this demand of being proper contradicts to that of being compact Consequently in the parameter type the better the compactness of the assumed structure that is proper the better the learning machine This is the most elementary conception when we design learning machines      Universality and Ordinary Neural Networks Now suppose that a sufficient knowledge on f is given though J itself is unknown In this case it is comparatively easy to find out proper and compact structures of J In the alternative case however it is sometimes difficult A possible solution is to give up the compactness and assume an almighty structure that can cover various s A combination of some orthogonal bases of the infinite dimension is such a structure Neural networks   are its approximations obtained by truncating finitely the dimension for implementation   American Institute of Physics   \\x0c A main topic in designing neural networks is to establish such desirable structures of  This work includes developing practical procedures that compute values of coefficients from the observed samples Such discussions are flourishing since  while many efficient methods have been proposed Recently even hardware units computing coefficients in parallel for speedup are sold eg ANZA Mark III Odyssey and E Nevertheless in neural networks there always exists a danger of some error remaining eternally in estimating  Precisely speaking suppose that a combination of the bases of a finite number can define a structure of  essentially In other words suppose that F   or  is located near F In such case the estimation error is none or negligible However if  is distant from F the estimation error never becomes negligible Indeed many researches report that the following situation appears when  is too complex Once the estimation error converges to some value   as the number of samples increases it decreases hardly even though the dimension is heighten This property sometimes is a considerable defect of neural networks   Recursi ve Type The recursive type is founded on another methodology of learning that should be as follows At the initial stage of no sample the set Fa instead of notation F of candidates of I equals to the set of all mappings from X to Y After observing the first sample Xl Yl E X x Y Fa is reduced to Fi so that Ixt  Yl for any I E F After observing the second sample X Y E X x Y Fl is further reduced to F so that ixt  Yl and IX  Y for any I E F Thus the candidate set F becomes gradually small as observation of samples proceeds The after observing isamples which we write is one of the most likelihood estimation of  selected in fi Hence contrarily to the parameter type the recursive type guarantees surely that j approaches to  as the number of samples increases The recursive type if observes a sample x yd rewrites values lXS to Ixs for some xs correlated to the sample Hence this type has an architecture composed of a rule for rewriting and a free memory space Such architecture forms naturally a kind of database that builds up management systems of data in a selforganizing way However this database differs from ordinary ones in the following sense It does not only record the samples already observed but computes some estimation of lx for any x E X We call such database an associative database The first subject in constructing associative databases is how we establish the rule for rewri ting For this purpose we adap t a measure called the dissimilari ty Here a dissimilari ty means a mapping d  X x X  reals  O such that for any x x E X x X dx x   whenever lx t x However it is not necessarily defined with a single formula It is definable with for example a collection of rules written in forms of if  then   The dissimilarity d defines a structure of  locally in X x Y Hence even though the knowledge on f is imperfect we can reflect it on d in some heuristic way Hence contrarily to neural networks it is possible to accelerate the speed of learning by establishing d well Especially we can easily find out simple ds for those ls which process analogically information like a human See the applications in this paper And for such s the recursive type shows strongly its effectiveness We denote a sequence of observed samples by Xl Yd X Y One of the simplest constructions of associative databases after observing isamples i   is as follows  i  i  I  Algorithm  At the initial stage let So be the empty set For every i     let ilx for any x E X equal some y such that xy E Sl and  dx x   min yESt  dx x   Furthermore add x y to Sl to produce Sa ie S  Sl U x    yn  \\x0c  Another version improved to economize the memory is as follows  Algorithm  At the initial stage let So be composed of an arbitrary element in X x Y For every i   let iilex for any x E X equal some y such that x y E Sil and dx x   min  dx x   iiESl  Furthermore if iilXi # Yi then let Si  Sil or add Xi Yi to Sil to produce Si ie Si  Sil U Xi Yi In either construction ii approaches to f as i increases However the computation time grows proportionally to the size of Si The second subject in constructing associative databases is what addressing rule we should employ to economize the computation time In the subsequent chapters a construction of associative database for this purpose is proposed It manages data in a form of binary tree  SELFORGANIZATION OF ASSOCIATIVE DATABASE Given a sample sequence Xl Yl X Y   the algorithm for constructing associative database is as follows  Algorithm   Step IInitialization Let xroot yroot  Xl Yd Here x and y are variables assigned for respective nodes to memorize data Furthermore let t   Step  Increase t by  and put x in After reset a pointer n to the root repeat the following until n arrives at some terminal node ie leaf Notations nand dxt xn let n  n mean the descendant nodes of n n Otherwise let n n  If dx rn   Step  Display yIn as the related information Next put y in If yIn  y back to step  Otherwise first establish new descendant nodes n and n Secondly let  xn yIn xn yIn  xn yIn Xt y     Finally back to step  Here the loop of step  can be stopped at any time and also can be continued Now suppose that gate elements namely artificial synapses that play the role of branching by d are prepared Then we obtain a new style of neural network with gate elements being randomly connected by this algorithm  LETTER RECOGNITION Recen tly the vertical slitting method for recognizing typographic English letters  the elastic matching method for recognizing hand written discrete English letters  the global training and fuzzy logic search method for recognizing Chinese characters written in square styleS etc are published The selforganization of associative database realizes the recognition of handwritten continuous English letters  \\x0c   wn  NOV     xk Lat      dwlo  of   Fig  Source document   lOO  H  o  o Fig  Windowing          Number of samples  o          NUAlber of sampl es  Fig  An experiment result  An image scanner takes a document image Fig  The letter recognizer uses a parallelogram window that at least can cover the maximal letter Fig  and processes the sequence of letters while shifting the window That is the recognizer scans a word in a slant direction And it places the window so that its left vicinity may be on the first black point detected Then the window catches a letter and some part of the succeeding letter If recognition of the head letter is performed its end position namely the boundary line between two letters becomes known Hence by starting the scanning from this boundary and repeating the above operations the recognizer accomplishes recursively the task Thus the major problem comes to identifying the head letter in the window Considering it we define the following  Regard window images as xs and define X accordingly  For a x x E X x X denote by B a black point in the left area from the boundary on window image X Project each B onto window image x Then measure the Euclidean distance  between fj and a black point B on x being the closest to B Let dx x be the summation of s for all black points Bs on x divided by the number of Bs  Regard couples of the reading and the position of boundary as ys and define Y accordingly An operator teaches the recognizer in interaction the relation between window image and reading boundary with algorithm  Precisely if the recalled reading is incorrect the operator teaches a correct reading via the console Moreover if the boundary position is incorrect he teaches a correct position via the mouse Fig  shows partially a document image used in this experiment Fig  shows the change of the number of nodes and that of the recognition rate defined as the relative frequency of correct answers in the past  trials Speciiications of the window are height  dot width  dot and slant angular  deg In this example the levels of tree were distributed in  at time  and the recognition rate converged to about  Experimentally the recognition rate converges to about  in most cases and to  at a rare case However it does not attain  since eg c and e are not distinguishable because of excessive lluctuation in writing If the consistency of the x yrelation is not assured like this the number of nodes increases endlessly d Fig  Hence it is clever to stop the learning when the recognition rate attains some upper limit To improve further the recognition rate we must consider the spelling of words It is one of future subjects  \\x0c  OBSTACLE AVOIDING MOVEMENT Various systems of camera type autonomous mobile robot are reported flourishinglyO The system made up by the authors Fig  also belongs to this category Now in mathematical methodologies we solve usually the problem of obstacle avoiding movement as a cost minimization problem under some cost criterion established artificially Contrarily the selforganization of associative database reproduces faithfully the cost criterion of an operator Therefore motion of the robot after learning becomes very natural Now the length width and height of the robot are all about Om and the weight is about kg The visual angle of camera is about deg The robot has the following three factors of motion It turns less than deg advances less than m and controls speed less than kmh The experiment was done on the passageway of wid th m inside a building which the authors laboratories exist in Fig  Because of an experimental intention we arrange boxes smoking stands gas cylinders stools handcarts etc on the passage way at random We let the robot take an image through the camera recall a similar image and trace the route preliminarily recorded on it For this purpose we define the following  Let the camera face deg downward to take an image and process it through a low pass filter Scanning vertically the filtered image from the bottom to the top search the first point C where the luminance changes excessively Then su bstitu te all points from the bottom to C for white and all points from C to the top for black Fig  If no obstacle exists just in front of the robot the white area shows the free area where the robot can move around Regard binary  x dot images processed thus as xs and define X accordingly  For every x x E X x X let dx x be the number of black points on the exclusiveor image between x and X  Regard as ys the images obtained by drawing routes on images xs and define Y accordingly The robot superimposes on the current camera image x the route recalled for x and inquires the operator instructions The operator judges subjectively whether the suggested route is appropriate or not In the negative answer he draws a desirable route on x with the mouse to teach a new y to the robot This operation defines implicitly a sample sequence of x y reflecting the cost criterion of the operator  l    IibUBe         Roan        Stationary uni t  Fig  Configuration of autonomous mobile robot system    I        North   rmbi Ie unit robot    Roan  y  t  Fig  Experimental environment  \\x0c  Wall  Camera image  Preprocessing  A   fa    Preprocessing    O  Course suggest ion      Search  A  Fig  Processing for obstacle avoiding movement  x  Fig  Processing for position identification We define the satisfaction rate by the relative frequency of acceptable suggestions of route in the past  trials In a typical experiment the change of satisfaction rate showed a similar tendency to Fig  and it attains about  around time  Here notice that the rest  does not mean directly the percentage of collision In practice we prevent the collision by adopting some supplementary measure At time  the number of nodes was  and the levels of tree were distributed in  The proposed method reflects delicately various characters of operator For example a robot trained by an operator  moves slowly with enough space against obstacles while one trained by another operator  brushes quickly against obstacles This fact gives us a hint on a method of printing characters into machines POSITION IDENTIFICATION The robot can identify its position by recalling a similar landscape with the position data to a camera image For this purpose in principle it suffices to regard camera images and position data as xs and ys respectively However the memory capacity is finite in actual compu ters Hence we cannot but compress the camera images at a slight loss of information Such compression is admittable as long as the precision of position identification is in an acceptable area Thus the major problem comes to find out some suitable compression method In the experimental environment Fig  juts are on the passageway at intervals of m and each section between adjacent juts has at most one door The robot identifies roughly from a surrounding landscape which section itself places in And it uses temporarily a triangular surveying technique if an exact measure is necessary To realize the former task we define the following   Turn the camera to take a panorama image of deg Scanning horizontally the center line substitute the points where the luminance excessively changes for black and the other points for white Fig  Regard binary dot line images processed thus as xs and define X accordingly  For every x x E X x X project each black point A on x onto x And measure the Euclidean distance  between A and a black point A on x being the closest to A Let the summation of  be S Similarly calculate S by exchanging the roles of x and X Denoting the numbers of As and As respectively by nand n define  \\x0c  dx x       n n     Regard positive integers labeled on sections as ys cf Fig  and define Y accordingly In the learning mode the robot checks exactly its position with a counter that is reset periodically by the operator The robot runs arbitrarily on the passageways within m area and learns the relation between landscapes and position data Position identification beyond m area is achieved by crossing plural databases one another This task is automatic excepting the periodic reset of counter namely it is a kind of learning without teacher We define the identification rate by the relative frequency of correct recalls of position data in the past  trials In a typical example it converged to about  around time  At time  the number of levels was  and the levels oftree were distributed in  Since the identification failures of  can be rejected by considering the trajectory no pro blem arises in practical use In order to improve the identification rate the compression ratio of camera images must be loosened Such possibility depends on improvement of the hardware in the future Fig  shows an example of actual motion of the robot based on the database for obstacle avoiding movement and that for position identification This example corresponds to a case of moving from  to  in Fig  Here the time interval per frame is about sec     i         I  I       I   t    i       II  Fig  Actual motion of the robot  \\x0c  CONCLUSION A method of selforganizing associative databases was proposed with the application to robot eyesight systems The machine decomposes a global structure unknown into a set of local structures known and learns universally any inputoutput response This framework of problem implies a wide application area other than the examples shown in this paper A defect of the algorithm  of selforganization is that the tree is balanced well only for a subclass of structures of f A subject imposed us is to widen the class A probable solution is to abolish the addressing rule depending directly on values of d and instead to establish another rule depending on the distribution function of values of d It is now under investigation  REFERENCES  Hopfield J J and D W Tank Computing with Neural Circuit A Model  Science   pp   Rumelhart D E et al Learning Representations by BackPropagating Errors Nature   pp    Hull J J Hypothesis Generation in a Computational Model for Visual Word Recognition IEEE Expert Fall  pp   Kurtzberg J M Feature Analysis for Symbol Recognition by Elastic Matching IBM J Res Develop   pp    Wang Q R and C Y Suen Large Tree Classifier with Heuristic Search and Global Training IEEE Trans Pattern Anal  Mach Intell PAMI   pp   Brooks R A et al Self Calibration of Motion and Stereo Vision for Mobile Robots th Int Symp of Robotics Research  pp   Goto Y and A Stentz The CMU System for Mobile Robot Navigation  IEEE Int Conf on Robotics  Automation  pp   Madarasz R et al The Design of an Autonomous Vehicle for the Disabled IEEE Jour of Robotics  Automation RA   pp   Triendl E and D J Kriegman Stereo Vision and Navigation within Buildings  IEEE Int Conf on Robotics  Automation  pp   Turk M A et al Video RoadFollowing for the Autonomous Land Vehicle  IEEE Int Conf on Robotics  Automation  pp   \\x0c'"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['cleaned_paper_text']=df['cleaned_paper_text'].apply(lambda x : x.lower())"
      ],
      "metadata": {
        "id": "rPUeer_VOCOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['cleaned_paper_text'] = df['cleaned_paper_text'].apply(lambda x : ' '.join([w for w in x.split() if len(w)>3]))"
      ],
      "metadata": {
        "id": "hj7idXYnOHxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['cleaned_paper_text'] = df['cleaned_paper_text'].apply(lambda x : ' '.join([w for w in x.split() if not w in stop_words]))"
      ],
      "metadata": {
        "id": "mliuWyG6OWXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vect =TfidfVectorizer(stop_words=stop_words,max_features=1000)\n",
        "vect_text=vect.fit_transform(df['cleaned_paper_text'])"
      ],
      "metadata": {
        "id": "ggwikdKaOnW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.shape(vect_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tENIaly4PB2l",
        "outputId": "4ab59bce-25e4-4d98-a673-d99a1a55dfaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7241, 1000)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BACCJGIOPEuR",
        "outputId": "b8d391c3-46c2-464a-e9d6-792dda208a9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7241, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vhebivcpQOHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_clean = [doc.split() for doc in df['cleaned_paper_text']]  "
      ],
      "metadata": {
        "id": "ioS09qzukZnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Gensim\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "\n",
        "# Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)\n",
        "dictionary = corpora.Dictionary(doc_clean)\n",
        "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
        "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]"
      ],
      "metadata": {
        "id": "yPYk2zl5kZrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the object for LDA model using gensim library\n",
        "Lda = gensim.models.ldamodel.LdaModel\n",
        "\n",
        "# Running and Trainign LDA model on the document term matrix.\n",
        "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=1)"
      ],
      "metadata": {
        "id": "s9BbMRqikZuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ldamodel.print_topics(num_topics=10, num_words=10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnmavHdslNTM",
        "outputId": "aaa08dbb-dd04-44b7-e851-96301ddd8835"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, '0.009*\"model\" + 0.008*\"data\" + 0.007*\"learning\" + 0.006*\"training\" + 0.006*\"neural\" + 0.005*\"network\" + 0.005*\"using\" + 0.005*\"image\" + 0.004*\"networks\" + 0.004*\"models\"'), (1, '0.010*\"learning\" + 0.006*\"function\" + 0.005*\"algorithm\" + 0.005*\"state\" + 0.004*\"model\" + 0.004*\"time\" + 0.004*\"network\" + 0.004*\"using\" + 0.004*\"neural\" + 0.004*\"policy\"'), (2, '0.008*\"algorithm\" + 0.006*\"data\" + 0.006*\"learning\" + 0.006*\"model\" + 0.005*\"function\" + 0.005*\"problem\" + 0.004*\"matrix\" + 0.004*\"using\" + 0.004*\"number\" + 0.004*\"algorithms\"')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "ynMgwGmelxbI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}